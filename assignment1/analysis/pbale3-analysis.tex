 \documentclass[h]{article}
\usepackage[margin=0.5in]{geometry}
\usepackage{amsfonts} 
\usepackage{textcomp}
 
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float} 
\usepackage{flafter}
\graphicspath{ {../images/} }
\usepackage{adjustbox}


\newcommand{\cent}{\textcent \hspace{4pt}}
\title{CS 7641 Machine Learning \\ Assignment 1}
\date{Due February 4th, 2018 11:59pm}
\author{Philip Bale \\ pbale3}

\begin{document}

\maketitle2 most important plots: learning curve and model complexity curve

\section*{Classification Problems}
\subsection*{1) US Permanent Visa Applications}  
\subsubsection*{Overview}
The first classification problem revolves around classifying whether or not a 
person's US permanent visa application will be accepted or denied based on the parameters of 
their application.  Among the features used in the classifcation are:
\begin{itemize}
  \item Job features: Industry code, job class, wage rate, wage type
  \item Geographic features: Country of citizen and employer location
\end{itemize}
The classes observed for this dataset are simply 'approved' and 'denied'.  The 
dataset contains 374365 total samples.
\subsubsection*{Why is the dataset interesting?}
This dataset is interesting due to its potential to aid in the visa application 
process from a cost and time savings potential.  It could also enable confidence in those 
interested in applying for a US permanent visa but doubting their chances of 
acceptance.  At the end of the day, the goal is it to try to determine the application result 
before time, money, and other resources are spent.  As someone who has worked 
with a large number of first-generation visa holders and immigrants, I am 
extremely interested in building tools to help others to achieve the same.
\\ \\
From a machine learning perspective, the dataset is incredibly interesting due 
to its wide variety of features and the variety of values those features can take. 
 An immense number of job types, wage rates, and citizenships alone create an 
 extremely diverse dataset.  Additionally, the number of samples available 
 provide a comprehensive picture of historical data, lending towards greater 
 cofidence in training and testing rates.

\subsection*{2) Home Sale Price Predictions}  
\subsubsection*{Overview}
The second classification problem revolves around classifying a home's price 
bracket based upon the various characteristics of the home.  Among the features 
used in the classification are :
\begin{itemize}
  \item Subjective measurements: Exterior condition, house style, overall quality rating, and overall condition
  \item Objective measurements: Type of dwelling, building type, lot size, neighborhood, year built, and year sold
\end{itemize}
After an initial review of the dataset, the classes were defined as pricing 
brackets divided into 100k groups.  I.e: 0-100k, 100k-200k, 200k-300k, etc.  The 
dataset contains 1451 samples.  An additional dataset containing another 1400 
testing samples exists but was not used as it contains unclassified sale 
prices.  It will, however, prove useful for unsupervised learning.â€¨\subsubsection*{Why is the dataset interesting?}
This dataset is interesting for two primary reasons: real-world applicability 
and participating in a Kaggle challenge.  First, modeling home prices is both a 
difficult and lucrative task.  If one can succesfully model home sale prices on 
large sets of data, he/she can make large amounts of money investing in real 
estate when he/she detects outliers in listed price vs. what it is expected to sell for.  
This applies to flipping, investing, and remodeling.  Second, the dataset is 
part of an ongoing Kaggle competition that does not have a winning solution yet. 
 By taking part of the competition, the dataset presents the opportunity to work 
 towards a winning solution and advance ones algorithms over time. 
 \\ \\
 Houses can have a very large amount of features--with a large amount of variety 
 in the individual features.  Similarly, housing is prone to personal taste and 
 frequent need for upgrades/modernization.  In such, I believe price estimation is an 
 excellent problem, full of depth and complexity, that is suitable for a machine 
 learning approach.
 
  \section*{General Data Processing}
  The datasets I used were both relatively clean to begin with.  One small 
  problem, however, was that a lot of my features on both datasets were 
  text-based.  To transform the features into numeric values suitable for the 
  machine learning algorithms, I used a label encoder built into ScikitLearn.
  \\ \\
  I also did a small amount of preprocessing of the data to make it more 
  suitable for classification.  I dropped all unnecessary columns to help 
  speed up with data processing in general--which proved immensely helpful when 
  dealing with the more computationall intensive algorithms.  In the 
  case of home prices, I precalculated the brackets based on the 'sale price' data label.  
  In the case of visa applications, I pregrouped the case outcome so that 
  results such as 'certified' and 'certified withdrawn' are both concerned as 
  'approved' conditions whereas 'denied', 'invalidated', and 'rejected' all 
  resolve to 'denied'.
  
 \section*{#1: Decision Trees}
 A decision tree classifier was the first algorithm applied to the datasets.  
 Various values of max\_depth were tested as a means of pruning unnecessary 
 leaves.  Similarly, a grid search was used to test whether a 'gini' or 
 'entropy' criterion was more effective.

\subsection*{US Permanent Visa Data}
\begin{figure}[H]
\minipage{0.69\textwidth}
\begin{tabular}{ | c | c  | c | c | c | c | c |} 
\hline
\textbf{Depth} & \textbf{Criterion} & \textbf{Tree Size} & \textbf{Train \%} & \textbf{Train Time} & \textbf{Test \%} & \textbf{Test Time}   \\
\hline
1 & gini & 3 & 0.7657 & 0.1212 & 0.7680 & 0.0010 \\ \hline
3 & gini & 15 & 0.6775 & 0.1417 & 0.6749 & 0.0009 \\ \hline
6 & entropy & 105 & 0.7121 & 0.2605 & 0.6924 & 0.0011 \\ \hline
10 & gini & 889 & 0.7584 & 0.3192 & 0.7072 & 0.0011 \\ \hline
15 & gini & 3013 & 0.8628 & 0.4199 & 0.7582 & 0.0014 \\ \hline
20 & gini & 4751 & 0.9299 & 0.4517 & 0.7917 & 0.0010 \\ \hline
25 & gini & 5349 & 0.9545 & 0.5284 & 0.8032 & 0.0010 \\ \hline
35 & entropy & 5349 & 0.9574 & 0.5116 & 0.8081 & 0.0010 \\ \hline
\end{tabular}
\caption*{Results at multiple depths for best criterion via grid search}
\endminipage\hfill
\minipage{0.29\textwidth}
\begin{flushright}
\begin{tabular}{ | c | c | c  | } 
\hline
 & Accepted & Denied  \\
\hline
Accepted & 96 & 135 \\ \hline
Denied & 217 & 1376 \\ \hline
\end{tabular}
\caption*{Test Data Confusion matrix}
\end{flushright}
\endminipage\hfill

\end{figure}

\begin{figure}[H]
  \minipage{0.32\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{1_curve_dtree3.png} 
      \caption*{Learning Curve for max\_depth = 3} 
   \endminipage\hfill
   \minipage{0.32\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{1_curve_dtree10.png} 
      \caption*{Learning Curve for max\_depth = 10} 
   \endminipage\hfill
   \minipage{0.32\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{1_curve_dtree20.png} 
      \caption*{Learning Curve for max\_depth = 20} 
   \endminipage\hfill
\end{figure}

\subsection*{Housing Prices Data}
\begin{figure}[H]
\minipage{0.69\textwidth}
\begin{tabular}{ | c | c  | c | c | c | c | c |} 
\hline
\textbf{Depth} & \textbf{Criterion} & \textbf{TreeSize} & \textbf{Train \%} & \textbf{Train Time} & \textbf{Test \%} & \textbf{Test Time}   \\
\hline
1 & gini & 3 & 0.0912 & 0.0355 & 0.0890 & 0.0006 \\ \hline
3 & entropy & 15 & 0.5816 & 0.0401 & 0.5890 & 0.0003 \\ \hline
6 & entropy & 91 & 0.6820 & 0.0442 & 0.6438 & 0.0003 \\ \hline
10 & gini & 331 & 0.8575 & 0.0566 & 0.6918 & 0.0006 \\ \hline
15 & gini & 593 & 0.9854 & 0.0695 & 0.7603 & 0.0003 \\ \hline
20 & gini & 643 & 1.0000 & 0.0559 & 0.7603 & 0.0007 \\ \hline
25 & gini & 639 & 1.0000 & 0.0550 & 0.7603 & 0.0006 \\ \hline
35 & gini & 639 & 1.0000 & 0.0570 & 0.7603 & 0.0006 \\ \hline

\end{tabular}
\caption*{Results at multiple depths for best criterion via grid search}
\endminipage\hfill
\minipage{0.29\textwidth}
\begin{flushright}
\begin{tabular}{ | c | c | c  | c | c | c | } 
\hline
 & 0-1 & 1-2 & 2-3 & 3-4 & 4-5  \\
\hline
0-1 & 2 & 6 & 0 & 0 & 0 \\ \hline 
1-2 & 4 & 87 & 7 & 0 & 0 \\ \hline 
2-3 & 0 & 8 & 15 & 1 & 2 \\ \hline 
3-4 & 0 & 0 & 6 & 3 & 0 \\ \hline 
4-5 & 0 & 0 & 2 & 0 & 3 \\ \hline 
\end{tabular}
\caption*{  \ \ \  Test Data Confusion matrix  \\  \ \ \ \ \ \ (classes in 100ks)}
\end{flushright}
\endminipage\hfill

\end{figure}

\begin{figure}[H]
  \minipage{0.32\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{2_curve_dtree3.png} 
      \caption*{Learning Curve for max\_depth = 3} 
   \endminipage\hfill
   \minipage{0.32\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{2_curve_dtree10.png} 
      \caption*{Learning Curve for max\_depth = 10} 
   \endminipage\hfill
   \minipage{0.32\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{2_curve_dtree20.png} 
      \caption*{Learning Curve for max\_depth = 20} 
   \endminipage\hfill
\end{figure}

\subsection*{Analysis for Decision Tree}
Overall the classifier worked quite well for both datasets, but was extremely prone to 
overfitting.  Examining the results of the decision tree classifier on the two 
datasets provides numerous observations and basis for analysis, which are provided below.

\subsubsection*{Effects of dataset size & cross validation:}
Above, learning curves are provided for both datasets.  It is immediately 
apparent that the size of the dataset greatly affects the performance of the 
algorithm--though diminishes over time. This makes sense for a few different 
reasons.  The more data we have to train on, the more likely it is that 
we see the full spectrum of possible variability.  Similarly, the broader the 
set of examples, the less biased our algorithm will be.  This is because if we 
only train on a few data samples, then our algorithm can only make decisions 
based on the features learned from the small, simple sample size--thus generating a bias (and therefore 
underfitting).
\\ \\ 
The learning curves for both datasets clearly level out as 
dataset size increases, demonstrating less variance with a more informed model.  
Similarly, cross-validation was used to normalize the training 
samples and smooth the learnign curve.  Without cross-validationg, the model was 
prone to unrepresentative, biased dataset samples during training and, in-turn, testing.
\subsubsection*{Tuning Parameters:}
The two main paremeters tuned were maximum tree depth and the split criterion.  
The split criterion measures the quality of a tree split.  Both 'gini' and 
'entropy' were tested and evaluated using a gridsearch.  For the large majority 
of trials, especially with larger and more complex trees, 'gini' was the more 
effective splitting criterion--though the results for the two were nearly 
identical.  If anything, Gini was more performant due to it's mathematical 
simplicity over the entropy formula.
\\ \\
Tree depth provided to be a singificant influencer over the performance of our 
model--especially for the housing dataset.  By allowing for a greater tree 
depth, we allow for a more complex tree with more possible decisions.  A problem 
emerges when the tree becomes too deep, however.  Instead of becoming more 
robust, the tree begins to overfit with very specific branches for very specific 
data items.  By analyzing the test \% vs. depth tradeoff, we can prune 
unnecessary tree depths for the optimal tree size.  This allowed for both a 
fast, accurate tree with a reasonable footprint.

\subsubsection*{Performance:}
Peformance varied for the two datasets across a few different domains.  In terms of runtime, housing prices took 
longer than the visa data to train (about 10x)--though both were extremely fast on a powerful laptop.
  This was mainly due to a larger dataset size.  As 
the depth of the tree increased, the training time also took longer for both datasets due to the
increased tree size and complexity(as can be seen from the table above).
\\ \\ 
In terms of accuracy, the visa data started out somewhat high but was able to gain about 4\% through 
optimizing the depth of the tree.  This is due to the fact that features like 
'salary' and 'job type' proved to heavily influence the approval process.  The 
housing data, however, saw more greater improvements as depth increased.    As a 
more diverse and variable dataset, the model was able to gain a lot of accuracy 
as the tree grew because it could accommodate more specific feature branches.  While the test data
started out at \textless 10\%, it was able to grow to approximately 76\%.   

\begin{figure}[H]
    \includegraphics[width=1.0\textwidth,keepaspectratio]{1_dtree.jpg} 
    \caption*{SampleDecision Tree: Permanent Visa with max\_depth of 7} 
\end{figure}

\section*{#2) AdaBoost - Boosted Trees}
A boosted decision tree classifer, Adaboost, was the second algorithm applied.  It used the same 
base decision tree as the first model algorithm.  Based on the results of the 
first model, we decided to stick with 'gini' as the criterion for faster 
training.

\subsection*{US Permanent Visa Data}
\begin{figure}[H]
\minipage{0.69\textwidth}
\begin{tabular}{ | c | c  | c | c | c | c | c |} 
\hline
\textbf{Depth} & \textbf{Learning Rate} & \textbf{\# Estimators} & \textbf{Train \%} & \textbf{Train Time} & \textbf{Test \%} & \textbf{Test Time}   \\
1 & 0.1 & 150 & 0.8705 & 24.1484 & 0.8662 & 0.0327 \\ \hline
3 & 0.1 & 100 & 0.8770 & 39.7266 & 0.8679 & 0.0216 \\ \hline
5 & 0.1 & 5 & 0.8761 & 53.8227 & 0.8706 & 0.0021 \\ \hline
10 & 0.1 & 150 & 0.9775 & 82.2192 & 0.8745 & 0.0504 \\ \hline
15 & 1 & 150 & 0.9775 & 94.5996 & 0.8701 & 0.0420 \\ \hline
20 & 1 & 150 & 0.9775 & 107.0742 & 0.8618 & 0.0452 \\ \hline
\hline


\end{tabular}
\caption*{Results at multiple depths for best learning rate/# estimators via grid search}
\endminipage\hfill
\end{figure}

\begin{figure}[H]
  \minipage{0.32\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{1_curve_boost3.png} 
      \caption*{Learning Curve for max\_depth = 3} 
   \endminipage\hfill
   \minipage{0.32\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{1_curve_boost10.png} 
      \caption*{Learning Curve for max\_depth = 10} 
   \endminipage\hfill
   \minipage{0.32\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{1_curve_boost20.png} 
      \caption*{Learning Curve for max\_depth = 20} 
   \endminipage\hfill
\end{figure}

\subsection*{Housing Prices Data}
\begin{figure}[H]
\minipage{0.69\textwidth}
\begin{tabular}{ | c | c  | c | c | c | c | c |} 
\hline
\textbf{Depth} & \textbf{Learning Rate} & \textbf{\# Estimators} & \textbf{Train \%} & \textbf{Train Time} & \textbf{Test \%} & \textbf{Test Time}   \\
\hline
1 & 0.1 & 15 & 0.6881 & 6.9413 & 0.6918 & 0.0018 \\ \hline
3 & 0.1 & 5 & 0.7724 & 8.2909 & 0.7945 & 0.0013 \\ \hline
5 & 0.1 & 3 & 0.8291 & 10.8559 & 0.7740 & 0.0011 \\ \hline
10 & 1 & 50 & 1.0000 & 13.6713 & 0.7945 & 0.0057 \\ \hline
15 & 1 & 100 & 1.0000 & 8.7526 & 0.8014 & 0.0145 \\ \hline
20 & 0.1 & 15 & 1.0000 & 0.6862 & 0.6781 & 0.0009 \\ \hline

\end{tabular}
\caption*{Results at multiple depths for best learning rate/# estimators via grid search}
\endminipage\hfill

\end{figure}

\begin{figure}[H]
  \minipage{0.32\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{2_curve_boost3.png} 
      \caption*{Learning Curve for max\_depth = 3} 
   \endminipage\hfill
   \minipage{0.32\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{2_curve_boost10.png} 
      \caption*{Learning Curve for max\_depth = 10} 
   \endminipage\hfill
   \minipage{0.32\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{2_curve_boost20.png} 
      \caption*{Learning Curve for max\_depth = 20} 
   \endminipage\hfill
\end{figure}

\subsection*{Analysis for AdaBoost Boosted Tree}
The AdaBoost boosted tree relates significantly to the original decision tree 
classifier.  It even uses the decision tree as its base classifier.  The main 
difference, however, exists in the boosting.  As discussed in class, a boosted 
tree is essentially an ensemble of weaker trees.  This ensemble works to 
classify with greater accuracy than the original, simplified tree. 
\\ \\
Evaluating the performance in the chart and graphs above, it is quickly noticed that the AdaBoost tree works
significantly better for both datasets.  For visa applications, it is approximately 7\% better in the optimal 
configuration.  For housing prices, it i approximately 4\% better.  Training 
times take signifcantly longer, which makes sense as the algorithm trains a much 
larger, ensemble classifier.  For example, the larger permanent visa dataset took more than a minute to
 train for depths greater than 5--compared to less than a second for a normal 
 tree classifier.
 \\ \\
 In terms of parameters, learning rate, depth, and # of estimators were all 
 tuned using a gridsearch to find an optimal model.  As # of estimators 
 increased, optimal learning rate tended to increase too.  For a deeper tree, 
 fewer estimators were needed because the optimal learning rate was achieved 
 sooner.  The learning rate, which effects the rate of contribution for each 
 classifier, was optimized between 0.1 and 1, where the learning rate trended 
 higher for deeper trees.
 \\ \\
 While both types of trees peformed well on the datasets, AdaBoost performed 
 exceptionally well.  I believe this is due to the rich set of features and logically 
 classifiable outcomes.  The AdaBoosted tree still overclassified at 
 increased depth.  By pruning depth to approximately 10 to 15, we were able to achieve 
 an optimal tree.
 
 \section*{#3) Neural Networks}
A neural network (using ScikitLearn's multi-layer perceptron classifier) was the third algorithm applied.  
A combination of different solvers, learning rates, and scaling was used to 
observe the functionality of the networks in regards to our dataset.


\begin{figure}[H]
  \minipage{0.49\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{1_nn.png} 
      \caption*{Permanent Visa Applicant NN Learning Curve} 
   \endminipage\hfill
   \minipage{0.49\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{2_nn.png} 
      \caption*{Housing Price NN Learning Curve} 
   \endminipage\hfill
\end{figure}

\begin{figure}[H]
  \minipage{1\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{1_nn_together.png} 
      \caption*{Permanent Visa Applicant NN Configurations Loss Curve} 
   \endminipage\hfill
   \minipage{1\textwidth}
      \includegraphics[width=1\textwidth,keepaspectratio]{2_nn_together.png} 
      \caption*{Housing Price NN Configurations Loss Curve} 
   \endminipage\hfill
\end{figure}

\begin{figure}[H]
\minipage{0.69\textwidth}
\begin{tabular}{ | c | c  | c | c | c | c | c |} 
\hline
\textbf{NN Config} & \textbf{Test Score} & \textbf{Loss} & \textbf{Train Time}   \\ 
\hline
constant learning-rate & 0.8642 & 0.3842 & 0.6121 \\ \hline
constant with momentum & 0.8663 & 0.3823 & 0.7120 \\ \hline
constant with Nesterov's momentum & 0.8668 & 0.3779 & 1.4836 \\ \hline
inv-scaling learning-rate & 0.8646 & 0.3905 & 0.2435 \\ \hline
inv-scaling with momentum & 0.8646 & 0.3842 & 0.3750 \\ \hline
inv-scaling with Nesterov's momentum & 0.8646 & 0.3848 & 0.2740 \\ \hline
adam & 0.8677 & 0.3650 & 2.1166 \\ \hline
\end{tabular}
\caption*{Permanent Visa results for various NN configs}
\endminipage\hfill
\end{figure}

\begin{figure}[H]
\minipage{0.69\textwidth}
\begin{tabular}{ | c | c  | c | c | c | c | c |} 
\hline
\textbf{NN Config} & \textbf{Test Score} & \textbf{Loss} & \textbf{Train Time}   \\ 
\hline
constant learning-rate & 0.7664 & 0.6301 & 0.4272  \\ \hline
constant with momentum & 0.7498 & 0.6058 & 0.0889 \\ \hline
constant with Nesterov's momentum & 0.7595 & 0.5669 & 0.1286 \\ \hline
inv-scaling learning-rate & 0.6278 & 1.0219 & 0.3932  \\ \hline
inv-scaling with momentum & 0.7009 & 0.7936 & 1.9339  \\ \hline
inv-scaling with Nesterov's momentum & 0.6278 & 1.2652 & 0.0205  \\ \hline
adam & 0.7795 & 0.5237 & 0.2093  \\ \hline
\end{tabular}
\caption*{Housing Price results for various NN configs}
\endminipage\hfill
\end{figure}



\end{document}