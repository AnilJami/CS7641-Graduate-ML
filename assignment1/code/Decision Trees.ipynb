{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded imports\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from random import sample\n",
    "from collections import defaultdict\n",
    "import graphviz \n",
    "import pandas\n",
    "\n",
    "print('Loaded imports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philip/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (0,1,2,3,4,5,6,7,10,11,16,17,20,21,22,25,26,27,28,29,30,31,32,33,34,35,36,37,39,40,41,42,43,44,45,47,48,49,50,51,52,53,55,56,57,58,59,60,61,63,64,65,66,68,69,70,71,72,73,74,77,79,80,81,82,83,84,85,86,87,88,89,90,92,93,94,95,100,101,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,153) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataframes\n"
     ]
    }
   ],
   "source": [
    "# h1b_df = pandas.read_csv('../data/h1b.csv')\n",
    "perm_df = pandas.read_csv('../data/perm.csv', thousands=\",\")\n",
    "housing_df = pandas.read_csv('../data/housing.csv')\n",
    "\n",
    "# h1b_df = h1b_df[['FULL_TIME_POSITION', 'YEAR', 'PREVAILING_WAGE', 'CASE_STATUS']]\n",
    "perm_df = perm_df[['naics_2007_us_code', 'wage_offer_from_9089', 'case_status', 'wage_offer_unit_of_pay_9089', 'country_of_citzenship', 'employer_state', 'pw_level_9089']]\n",
    "housing_columns = ['MSSubClass', 'LotArea', 'Neighborhood', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'ExterCond', 'CentralAir', 'YrSold']\n",
    "housing_df = housing_df[housing_columns + ['SalePrice']]\n",
    "\n",
    "# h1b_df = h1b_df.sample(n=100000)\n",
    "print('Loaded dataframes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding: country_of_citzenship\n",
      "['AFGHANISTAN' 'ALBANIA' 'ANGOLA' 'ARGENTINA' 'ARMENIA' 'AUSTRALIA'\n",
      " 'AUSTRIA' 'AZERBAIJAN' 'BAHAMAS' 'BAHRAIN' 'BANGLADESH' 'BARBADOS'\n",
      " 'BELARUS' 'BELGIUM' 'BELIZE' 'BHUTAN' 'BOLIVIA' 'BOSNIA AND HERZEGOVINA'\n",
      " 'BRAZIL' 'BULGARIA' 'BURKINA FASO' 'BURMA (MYANMAR)' 'CAMBODIA' 'CAMEROON'\n",
      " 'CANADA' 'CHILE' 'CHINA' 'COLOMBIA' 'COSTA RICA' 'CROATIA' 'CYPRUS'\n",
      " 'CZECH REPUBLIC' 'DENMARK' 'DOMINICA' 'DOMINICAN REPUBLIC' 'ECUADOR'\n",
      " 'EGYPT' 'EL SALVADOR' 'ESTONIA' 'ETHIOPIA' 'FIJI' 'FINLAND' 'FRANCE'\n",
      " 'GAMBIA' 'GEORGIA' 'GERMANY' 'GHANA' 'GREECE' 'GRENADA' 'GUATEMALA'\n",
      " 'GUINEA' 'GUYANA' 'HAITI' 'HONDURAS' 'HONG KONG' 'HUNGARY' 'ICELAND'\n",
      " 'INDIA' 'INDONESIA' 'IRAN' 'IRAQ' 'IRELAND' 'ISRAEL' 'ITALY' 'IVORY COAST'\n",
      " 'JAMAICA' 'JAPAN' 'JORDAN' 'KAZAKHSTAN' 'KENYA' 'KOSOVO' 'KUWAIT'\n",
      " 'KYRGYZSTAN' 'LAOS' 'LATVIA' 'LEBANON' 'LESOTHO' 'LIBERIA' 'LIBYA'\n",
      " 'LITHUANIA' 'MACEDONIA' 'MALAYSIA' 'MALI' 'MALTA' 'MAURITIUS' 'MEXICO'\n",
      " 'MOLDOVA' 'MONGOLIA' 'MOROCCO' 'NEPAL' 'NETHERLANDS' 'NEW ZEALAND'\n",
      " 'NICARAGUA' 'NIGERIA' 'NORTH KOREA' 'NORWAY' 'OMAN' 'PAKISTAN' 'PALESTINE'\n",
      " 'PANAMA' 'PARAGUAY' 'PERU' 'PHILIPPINES' 'POLAND' 'PORTUGAL' 'ROMANIA'\n",
      " 'RUSSIA' 'RWANDA' 'SAUDI ARABIA' 'SENEGAL' 'SERBIA AND MONTENEGRO'\n",
      " 'SIERRA LEONE' 'SINGAPORE' 'SLOVAKIA' 'SLOVENIA' 'SOUTH AFRICA'\n",
      " 'SOUTH KOREA' 'SPAIN' 'SRI LANKA' 'ST KITTS AND NEVIS' 'ST LUCIA'\n",
      " 'ST VINCENT' 'SUDAN' 'SWEDEN' 'SWITZERLAND' 'SYRIA' 'TAIWAN' 'TAJIKISTAN'\n",
      " 'TANZANIA' 'THAILAND' 'TOGO' 'TRINIDAD AND TOBAGO' 'TUNISIA' 'TURKEY'\n",
      " 'TURKMENISTAN' 'UGANDA' 'UKRAINE' 'UNITED ARAB EMIRATES' 'UNITED KINGDOM'\n",
      " 'UNITED STATES OF AMERICA' 'URUGUAY' 'UZBEKISTAN' 'VENEZUELA' 'VIETNAM'\n",
      " 'YEMEN' 'YUGOSLAVIA' 'ZAMBIA' 'ZIMBABWE']\n",
      "Encoding: employer_state\n",
      "['AK' 'AL' 'AR' 'AZ' 'CA' 'CO' 'CT' 'DC' 'DE' 'FL' 'GA' 'GU' 'HI' 'IA' 'ID'\n",
      " 'IL' 'IN' 'KS' 'KY' 'LA' 'MA' 'MD' 'ME' 'MI' 'MN' 'MO' 'MP' 'MS' 'MT' 'NC'\n",
      " 'ND' 'NE' 'NH' 'NJ' 'NM' 'NV' 'NY' 'OH' 'OK' 'OR' 'PA' 'PR' 'RI' 'SC' 'SD'\n",
      " 'TN' 'TX' 'UT' 'VA' 'VI' 'VT' 'WA' 'WI' 'WV' 'WY']\n",
      "Encoding: pw_level_9089\n",
      "['Level I' 'Level II' 'Level III' 'Level IV']\n",
      "   naics_2007_us_code  wage_offer_from_9089  case_status  \\\n",
      "0            541512.0               75629.0            1   \n",
      "1            562211.0               37024.0           -1   \n",
      "2            541330.0               47923.0            1   \n",
      "\n",
      "   wage_offer_unit_of_pay_9089  country_of_citzenship  employer_state  \\\n",
      "0                          2.0                      4              36   \n",
      "1                          2.0                    103              36   \n",
      "2                          2.0                     57              48   \n",
      "\n",
      "   pw_level_9089  \n",
      "0              1  \n",
      "1              0  \n",
      "2              0  \n",
      "['wage_offer_from_9089', 'naics_2007_us_code', 'wage_offer_unit_of_pay_9089', 'country_of_citzenship', 'employer_state', 'employer_state', 'pw_level_9089']\n",
      "case_status\n",
      "Index(['naics_2007_us_code', 'wage_offer_from_9089', 'case_status',\n",
      "       'wage_offer_unit_of_pay_9089', 'country_of_citzenship',\n",
      "       'employer_state', 'pw_level_9089'],\n",
      "      dtype='object')\n",
      "naics_2007_us_code             float64\n",
      "wage_offer_from_9089           float64\n",
      "case_status                      int64\n",
      "wage_offer_unit_of_pay_9089    float64\n",
      "country_of_citzenship            int64\n",
      "employer_state                   int64\n",
      "pw_level_9089                    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# format data\n",
    "datasetNum = 1\n",
    "\n",
    "df = None\n",
    "df_x = None\n",
    "df_y = None\n",
    "to_encode = []\n",
    "\n",
    "classifed_names = None\n",
    "\n",
    "if datasetNum == 0:\n",
    "    df = h1b_df.copy()\n",
    "    df_x = ['FULL_TIME_POSITION', 'YEAR', 'PREVAILING_WAGE']\n",
    "    df_y = 'CASE_STATUS'\n",
    "elif datasetNum == 1:\n",
    "    df = perm_df.copy()\n",
    "    df_x = [\n",
    "        'wage_offer_from_9089',\n",
    "        'naics_2007_us_code', 'wage_offer_unit_of_pay_9089', 'country_of_citzenship', 'employer_state', 'employer_state', 'pw_level_9089']\n",
    "    df_y = 'case_status'\n",
    "    to_encode = [ 'country_of_citzenship', 'employer_state', 'pw_level_9089']\n",
    "elif datasetNum == 2:\n",
    "    df = housing_df.copy()\n",
    "    df_x = housing_columns\n",
    "    df_y = 'price_bracket'\n",
    "    to_encode = housing_columns\n",
    "    \n",
    "le = preprocessing.LabelEncoder\n",
    "encoderDict = defaultdict(le)\n",
    "\n",
    "for column in to_encode:\n",
    "    print('Encoding: ' + column)\n",
    "    df[column] = df[column].dropna()\n",
    "    df = df[df[column].notnull()]\n",
    "    df[column] = encoderDict[column].fit_transform(df[column])\n",
    "    print(encoderDict[column].classes_)\n",
    "\n",
    "if datasetNum == 0:\n",
    "#     df['CITY'], df['STATE'] = df['WORKSITE'].str.split(',', 1).str\n",
    "#     df['STATE'] = df['STATE'].str.strip()\n",
    "#     df['STATE'].apply(str)\n",
    "\n",
    "    df.loc[(df['FULL_TIME_POSITION'] == 'Y'), 'FULL_TIME_POSITION'] = 1\n",
    "    df.loc[(df['FULL_TIME_POSITION'] == 'N'), 'FULL_TIME_POSITION'] = -1\n",
    "    \n",
    "    df = df[~df['CASE_STATUS'].str.contains(\"WITHDRAWN\", na=True)]\n",
    "    df = df[~df['CASE_STATUS'].str.contains(\"PENDING\")]\n",
    "    df = df[~df['CASE_STATUS'].str.contains(\"INVALIDATED\")]\n",
    "    df.loc[(df['CASE_STATUS'] == 'CERTIFIED'), 'CASE_STATUS'] = 1\n",
    "    df.loc[(df['CASE_STATUS'] == 'CERTIFIED-WITHDRAWN'), 'CASE_STATUS'] = 1\n",
    "    df.loc[(df['CASE_STATUS'] == 'DENIED'), 'CASE_STATUS'] = -1\n",
    "    df.loc[(df['CASE_STATUS'] == 'REJECTED'), 'CASE_STATUS'] = -1\n",
    "    \n",
    "elif datasetNum == 1:\n",
    "    df = df[~df['case_status'].str.contains(\"Withdrawn\", na=False)]\n",
    "    df.loc[(df['case_status'].str.contains('Certified', na=False)), 'case_status'] = 1\n",
    "    df.loc[(df['case_status'].str.contains('Denied', na=False)), 'case_status'] = -1\n",
    "    \n",
    "    df.loc[(df['wage_offer_unit_of_pay_9089'].str.contains('yr', na=False)), 'wage_offer_unit_of_pay_9089'] = 2\n",
    "    df.loc[(df['wage_offer_unit_of_pay_9089'].str.contains('hr', na=False)), 'wage_offer_unit_of_pay_9089'] = 1\n",
    "    classifed_names = ['denied', 'approved']\n",
    "elif datasetNum == 2:\n",
    "    df['price_bracket'] = df['SalePrice'].copy().astype(int)\n",
    "    \n",
    "    classifed_names = []\n",
    "    for bracket in range(0, 15):\n",
    "        # Each bracket worth 75k\n",
    "        bracket_width = 100000\n",
    "        price_min = bracket * bracket_width\n",
    "#         print(str(bracket) +': '+ str(price_min))\n",
    "        price_max = price_min + bracket_width\n",
    "        classifed_names.append(str(price_min) + '-' + str(price_max))\n",
    "        df.loc[(df['SalePrice'] >= price_min), 'price_bracket'] = bracket\n",
    "    \n",
    "df = df.apply(lambda x: pandas.to_numeric(x.astype(str).str.replace(',',''), errors='coerce'))\n",
    "\n",
    "print(df[:3])\n",
    "# indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "# df = df[indices_to_keep]\n",
    "\n",
    "print(df_x)\n",
    "print(df_y)\n",
    "\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 127652\n",
      "Features: ['wage_offer_from_9089', 'naics_2007_us_code', 'wage_offer_unit_of_pay_9089', 'country_of_citzenship', 'employer_state', 'employer_state', 'pw_level_9089']\n",
      "Trying to classify: case_status\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()  \n",
    "# sample = df.sample(n=10000)\n",
    "\n",
    "# Shuffle\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print('Dataset size: ' + str(df.size))\n",
    "print('Features: ' + str(df_x))\n",
    "print('Trying to classify: ' + df_y)\n",
    "\n",
    "X = df.loc[:, df_x]\n",
    "y = df.loc[:, df_y]\n",
    "\n",
    "trainTest = train_test_split(X, y, test_size=0.1, train_size=0.9, random_state=0)\n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, learning_curve\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cross_validations = 4\n",
    "train_sizes_base = [50, 60, 80, 100, 125, 150, 200, 300, 400, 500, 600, 700, 800]\n",
    "\n",
    "def plot_learning_curve(title, cv_curve):\n",
    "#     _, _, test_scores_base = base_curve\n",
    "    train_sizes, train_scores, test_scores = cv_curve\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    \n",
    "#     test_scores_base_mean = np.mean(test_scores_base, axis=1)\n",
    "#     test_scores_base_std = np.std(test_scores_base, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.ylim((.4, 1.01))\n",
    "    \n",
    "    if datasetNum == 1:\n",
    "        plt.ylim((.55, 1.01))\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    \n",
    "#     plt.fill_between(train_sizes, test_scores_base_mean - test_scores_base_std,\n",
    "#                      test_scores_base_mean + test_scores_base_std, alpha=0.1, color=\"b\")\n",
    "    \n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    \n",
    "    \n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "#     plt.plot(train_sizes, test_scores_base_mean, 'o-', color=\"b\",\n",
    "#              label=\"Test Score without CV\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Test Score with CV\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "def crossValidateAndTest(name, clf, params, trainTest, scaler=None, plot=True):\n",
    "    X_train, X_test, y_train, y_test = trainTest.copy()\n",
    "    print('Name: ' + name)\n",
    "    if not scaler is None:\n",
    "        scaler.fit(X_train) \n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "#     base_clf = GridSearchCV(clf, param_grid=params, refit=True, cv=None)\n",
    "    cv_clf = GridSearchCV(clf, param_grid=params, refit=True, cv=cross_validations)\n",
    "    \n",
    "#     base_clf.fit(X_train, y_train)\n",
    "    \n",
    "    start = datetime.now()\n",
    "    cv_clf.fit(X_train, y_train)\n",
    "    end = datetime.now()\n",
    "    train_time = (end - start).total_seconds()\n",
    "    train_score = cv_clf.score(X_train, y_train)\n",
    "    print('Train time: ' + str(train_time))\n",
    "    print('Train score: ' + str(train_score))\n",
    "    \n",
    "    start = datetime.now()\n",
    "    test_score = cv_clf.score(X_test, y_test)\n",
    "    end = datetime.now()\n",
    "    test_time = (end - start).total_seconds()\n",
    "    print('Test time: ' + str(test_time))\n",
    "    print('Test score: ' + str(test_score))\n",
    "    \n",
    "    y_predict = cv_clf.predict(X_test)\n",
    "    confusion_results = confusion_matrix(y_test, y_predict)\n",
    "    print(confusion_results)\n",
    "    \n",
    "    \n",
    "#     base_estimator = base_clf.best_estimator_\n",
    "    cv_estimator = cv_clf.best_estimator_\n",
    "    \n",
    "    print(\"Best params: \" + str(cv_clf.best_params_))\n",
    "    \n",
    "    table_output = '{:.4f} & {:.4f} & {:.4f} & {:.4f}'.format(train_score, train_time, test_score, test_time)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     score = optimized_clf.score(X_test, y_test)\n",
    "     \n",
    "    additional_sizes = []\n",
    "    if datasetNum == 1:\n",
    "        additional_sizes = [1000, 1500, 2000, 2500, 3000]\n",
    "        \n",
    "    all_sizes = train_sizes_base + additional_sizes\n",
    "#     base_curve = learning_curve(base_estimator, X_train, y_train, cv=None, train_sizes=all_sizes)\n",
    "\n",
    "    if plot:\n",
    "        cv_curve = learning_curve(cv_estimator, X_train, y_train, cv=cross_validations, train_sizes=all_sizes)\n",
    "\n",
    "    #     plot = plot_learning_curve(name, base_curve, cv_curve)\n",
    "        plot = plot_learning_curve(name, cv_curve) \n",
    "\n",
    "    \n",
    "    return (cv_estimator, cv_clf, table_output)\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "params = { 'criterion':['gini','entropy'] }\n",
    "\n",
    "for i in [1, 3, 6, 10, 15, 20, 25, 35, 50]: \n",
    "    clf = tree.DecisionTreeClassifier(max_depth=i, class_weight='balanced', splitter='best', min_samples_leaf=1)\n",
    "    output, output_clf, table_output = crossValidateAndTest('Decision Tree: ' + str(i), clf, params, trainTest)\n",
    "    tree_size = output.tree_.node_count\n",
    "    print('{} & {} & {} & '.format(i, output_clf.best_params_['criterion'], tree_size) + table_output + ' \\\\\\\\ \\\\hline') \n",
    "    \n",
    "    print()\n",
    "#     clf = clf.fit(X_train, y_train)\n",
    "\n",
    "#     y_predict = clf.predict(X_test)\n",
    "#     scores = cross_val_score(clf, X, y)\n",
    "#     print(str(i) + ': ' + str(accuracy_score(y_test, y_predict)) + '   |   ' + str(scores.mean()))\n",
    "    if i == 7:\n",
    "        dot_data = tree.export_graphviz(output, out_file=None, \n",
    "                             feature_names=df_x,  \n",
    "                             class_names=classifed_names,  \n",
    "                             filled=True, rounded=True,  \n",
    "                             special_characters=True)  \n",
    "        graph = graphviz.Source(dot_data).view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "estimators = [1, 3, 5, 15, 50, 100, 150]\n",
    "learning_rate = [.1, 1, 10]\n",
    "\n",
    "for i in [1, 3, 5, 10, 15, 20]: \n",
    "    clf_base = tree.DecisionTreeClassifier(max_depth=i, criterion='gini', splitter='best')\n",
    "    clf = AdaBoostClassifier(base_estimator=clf_base)\n",
    "    output, output_clf, table_output = crossValidateAndTest('Adaboost: ' + str(i), clf, {'n_estimators': estimators, 'learning_rate': learning_rate}, trainTest)\n",
    "    tree_count = len(output)\n",
    "    print('{} & {} & {} & '.format(i, output_clf.best_params_['learning_rate'], output_clf.best_params_['n_estimators']) + table_output + ' \\\\\\\\ \\\\hline') \n",
    "    \n",
    "# for estimator in estimators:\n",
    "#     clf = AdaBoostClassifier(n_estimators=estimator)\n",
    "    \n",
    "#     crossValidateAndTest('Adaboost: ' + str(estimator), clf, {}, trainTest)\n",
    "    \n",
    "#     clf = clf.fit(X_train, y_train)\n",
    "\n",
    "#     y_predict = clf.predict(X_test)\n",
    "#     scores = cross_val_score(clf, X, y)\n",
    "#     print(str(estimator) + ': ' + str(accuracy_score(y_test, y_predict)) + '   |   ' + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "\n",
    "alphas = [0.0001, ]#0.005, 0.001]\n",
    "learning_rate_inits = [ 0.001, ]#.01, .1, .2]\n",
    "activations = ['relu']\n",
    "layers = []\n",
    "\n",
    "for i in [5,10,100]:\n",
    "    for j in [1, 3]:\n",
    "        layers.append((i, j))\n",
    "        \n",
    "# params = { 'alpha': alphas, 'learning_rate_init': learning_rate_init}\n",
    "params = {}\n",
    "\n",
    "print(\"Starting neural networks\")\n",
    "for alpha in alphas:\n",
    "    for learning_rate_init in learning_rate_inits:\n",
    "        clf = MLPClassifier(solver='adam', max_iter=2000, random_state=7, batch_size='auto')\n",
    "        plot = True\n",
    "        output, output_clf, table_output = crossValidateAndTest('Adam NN: ' + str(alpha) + '/' + str(learning_rate_init), clf, params, trainTest, StandardScaler(), plot)\n",
    "        print(str(output.n_iter_))\n",
    "        print('{} & {} & '.format(alpha, learning_rate_init) + table_output + ' \\\\\\\\ \\\\hline') \n",
    "        \n",
    "        clf = MLPClassifier(solver='sgd', max_iter=2000, random_state=7, batch_size='auto')\n",
    "        plot = True\n",
    "        output, output_clf, table_output = crossValidateAndTest('SGD NN: ' + str(alpha) + '/' + str(learning_rate_init), clf, params, trainTest, StandardScaler(), plot)\n",
    "        print(str(output.n_iter_))\n",
    "        print('{} & {} & '.format(alpha, learning_rate_init) + table_output + ' \\\\\\\\ \\\\hline') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for iteration_count in iterations:\n",
    "#     for layer_size_1 in layer_sizes_1:\n",
    "#         for layer_size_2 in layer_sizes_2:\n",
    "#             clf = MLPClassifier(solver='sgd', activation='relu', max_iter=iteration_count, alpha=0.001, batch_size=200, hidden_layer_sizes=(layer_size_1, layer_size_2), random_state=42)\n",
    "#             scaler.fit(X_train) \n",
    "\n",
    "#             X_train_new = scaler.transform(X_train)\n",
    "#             X_test_new = scaler.transform(X_test)\n",
    "#             clf.fit(X_train_new, y_train)\n",
    "#             y_predict = clf.predict(X_test_new)\n",
    "#             print(str(iteration_count) + ':' + str((layer_size_1, layer_size_2)) + ': ' + str(accuracy_score(y_test, y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "# different learning rate schedules and momentum parameters\n",
    "params = [{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0,\n",
    "           'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
    "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
    "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,\n",
    "           'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,\n",
    "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,\n",
    "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'adam', 'learning_rate_init': 0.01}]\n",
    "\n",
    "labels = [\"constant learning-rate\", \"constant with momentum\",\n",
    "          \"constant with Nesterov's momentum\",\n",
    "          \"inv-scaling learning-rate\", \"inv-scaling with momentum\",\n",
    "          \"inv-scaling with Nesterov's momentum\", \"adam\"]\n",
    "\n",
    "plot_args = [{'c': 'red', 'linestyle': '-'},\n",
    "             {'c': 'green', 'linestyle': '-'},\n",
    "             {'c': 'blue', 'linestyle': '-'},\n",
    "             {'c': 'red', 'linestyle': '--'},\n",
    "             {'c': 'green', 'linestyle': '--'},\n",
    "             {'c': 'blue', 'linestyle': '--'},\n",
    "             {'c': 'black', 'linestyle': '-'}]\n",
    "\n",
    "\n",
    "def plot_on_dataset(X, y, ax, name):\n",
    "    # for each dataset, plot learning for each learning strategy\n",
    "    print(\"\\nlearning on dataset %s\" % name)\n",
    "    ax.set_title(name)\n",
    "#     ax.xlabel(\"# Iterations\")\n",
    "#     ax.ylabel(\"Loss\")\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    mlps = []\n",
    "    if name == \"digits\":\n",
    "        # digits is larger but converges fairly quickly\n",
    "        max_iter = 15\n",
    "    else:\n",
    "        max_iter = 1000\n",
    "\n",
    "    for label, param in zip(labels, params):\n",
    "        print(\"training: %s\" % label)\n",
    "        mlp = MLPClassifier(verbose=0, random_state=0,\n",
    "                            max_iter=max_iter, **param)\n",
    "        \n",
    "        start = datetime.now()\n",
    "        mlp.fit(X, y)\n",
    "        end = datetime.now()\n",
    "        train_time = (end - start).total_seconds()\n",
    "        \n",
    "        start = datetime.now()\n",
    "        score = mlp.score(X, y)\n",
    "        end = datetime.now()\n",
    "        score_time = (end - start).total_seconds()\n",
    "        \n",
    "        table_output = '{} & {:.4f} & {:.4f} & {:.4f}'.format(label, score, mlp.loss_, train_time)\n",
    "        print(table_output)\n",
    "\n",
    "        \n",
    "        mlps.append(mlp)\n",
    "        print(\"Training set score: %f\" % score)\n",
    "        print(\"Training set loss: %f\" % mlp.loss_)\n",
    "    for mlp, label, args in zip(mlps, labels, plot_args):\n",
    "        ax.plot(mlp.loss_curve_, label=label, **args)\n",
    "        \n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(15, 5))\n",
    "# load / generate some toy datasets\n",
    "\n",
    "ax = axes\n",
    "plot_on_dataset(X, y, ax, 'Test')\n",
    "\n",
    "plt.xlabel(\"# Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "fig.legend(ax.get_lines(), labels, ncol=3, loc=\"upper center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: SVC1\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "clf_1 = svm.SVC(kernel='sigmoid', max_iter=20000)\n",
    "output, output_clf, table_output = crossValidateAndTest('SVC1', clf_1, {}, trainTest, StandardScaler())\n",
    "print('{} & '.format(gamma) + table_output + ' \\\\\\\\ \\\\hline') \n",
    "\n",
    "clf_2 = svm.SVC(kernel='linear', max_iter=20000)\n",
    "crossValidateAndTest('SVC2', clf_2, {}, trainTest, StandardScaler())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors\n",
    "\n",
    "print('Running knn')\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "k_vals = [1, 2, 3, 4, 5, 10, 15, 20, 30, 50]\n",
    " \n",
    "for k in k_vals:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(k)\n",
    "    crossValidateAndTest('KNN: ' + str(k), clf, {'weights': ['uniform', 'distance']}, trainTest)\n",
    "\n",
    "        \n",
    "#         clf.fit(X_train, y_train)\n",
    "        \n",
    "#         y_predict = clf.predict(X_test)\n",
    "#         scores = cross_val_score(clf, X, y)\n",
    "#         print(str(k) + ':' + weight + ': ' + str(accuracy_score(y_test, y_predict)) + '   |   ' + str(scores.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
