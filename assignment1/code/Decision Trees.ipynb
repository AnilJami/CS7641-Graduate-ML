{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded imports\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from random import sample\n",
    "from collections import defaultdict\n",
    "import graphviz \n",
    "import pandas\n",
    "\n",
    "print('Loaded imports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philip/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (0,1,2,3,4,5,6,7,10,11,16,17,20,21,22,25,26,27,28,29,30,31,32,33,34,35,36,37,39,40,41,42,43,44,45,47,48,49,50,51,52,53,55,56,57,58,59,60,61,63,64,65,66,68,69,70,71,72,73,74,77,79,80,81,82,83,84,85,86,87,88,89,90,92,93,94,95,100,101,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,153) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataframes\n"
     ]
    }
   ],
   "source": [
    "h1b_df = pandas.read_csv('../data/h1b.csv')\n",
    "perm_df = pandas.read_csv('../data/perm.csv', thousands=\",\")\n",
    "\n",
    "h1b_df = h1b_df[['FULL_TIME_POSITION', 'YEAR', 'PREVAILING_WAGE', 'CASE_STATUS']]\n",
    "perm_df = perm_df[['naics_2007_us_code', 'wage_offer_from_9089', 'case_status', 'wage_offer_unit_of_pay_9089', 'country_of_citzenship', 'employer_state', 'pw_level_9089']]\n",
    "\n",
    "# h1b_df = h1b_df.sample(n=100000)\n",
    "print('Loaded dataframes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding: country_of_citzenship\n",
      "Encoding: employer_state\n",
      "Encoding: pw_level_9089\n",
      "['wage_offer_from_9089', 'naics_2007_us_code', 'wage_offer_unit_of_pay_9089', 'country_of_citzenship', 'employer_state', 'employer_state', 'pw_level_9089']\n",
      "case_status\n",
      "Index(['naics_2007_us_code', 'wage_offer_from_9089', 'case_status',\n",
      "       'wage_offer_unit_of_pay_9089', 'country_of_citzenship',\n",
      "       'employer_state', 'pw_level_9089'],\n",
      "      dtype='object')\n",
      "naics_2007_us_code             float64\n",
      "wage_offer_from_9089           float64\n",
      "case_status                      int64\n",
      "wage_offer_unit_of_pay_9089    float64\n",
      "country_of_citzenship            int64\n",
      "employer_state                   int64\n",
      "pw_level_9089                    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# format data\n",
    "firstDataset = False\n",
    "\n",
    "df = None\n",
    "df_x = None\n",
    "df_y = None\n",
    "to_encode = []\n",
    "\n",
    "if firstDataset:\n",
    "    df = h1b_df.copy()\n",
    "    df_x = ['FULL_TIME_POSITION', 'YEAR', 'PREVAILING_WAGE']\n",
    "    df_y = 'CASE_STATUS'\n",
    "else:\n",
    "    df = perm_df.copy()\n",
    "    df_x = [\n",
    "        'wage_offer_from_9089',\n",
    "        'naics_2007_us_code', 'wage_offer_unit_of_pay_9089', 'country_of_citzenship', 'employer_state', 'employer_state', 'pw_level_9089']\n",
    "    df_y = 'case_status'\n",
    "    to_encode = [ 'country_of_citzenship', 'employer_state', 'pw_level_9089']\n",
    "    \n",
    "    \n",
    "le = preprocessing.LabelEncoder\n",
    "encoderDict = defaultdict(le)\n",
    "\n",
    "for column in to_encode:\n",
    "    print('Encoding: ' + column)\n",
    "    df[column] = df[column].dropna()\n",
    "    df = df[df[column].notnull()]\n",
    "    df[column] = encoderDict[column].fit_transform(df[column])\n",
    "#     print(encoderDict[column].classes_)\n",
    "\n",
    "if firstDataset:\n",
    "#     df['CITY'], df['STATE'] = df['WORKSITE'].str.split(',', 1).str\n",
    "#     df['STATE'] = df['STATE'].str.strip()\n",
    "#     df['STATE'].apply(str)\n",
    "\n",
    "    df.loc[(df['FULL_TIME_POSITION'] == 'Y'), 'FULL_TIME_POSITION'] = 1\n",
    "    df.loc[(df['FULL_TIME_POSITION'] == 'N'), 'FULL_TIME_POSITION'] = -1\n",
    "    \n",
    "    df = df[~df['CASE_STATUS'].str.contains(\"WITHDRAWN\", na=True)]\n",
    "    df = df[~df['CASE_STATUS'].str.contains(\"PENDING\")]\n",
    "    df = df[~df['CASE_STATUS'].str.contains(\"INVALIDATED\")]\n",
    "    df.loc[(df['CASE_STATUS'] == 'CERTIFIED'), 'CASE_STATUS'] = 1\n",
    "    df.loc[(df['CASE_STATUS'] == 'CERTIFIED-WITHDRAWN'), 'CASE_STATUS'] = 1\n",
    "    df.loc[(df['CASE_STATUS'] == 'DENIED'), 'CASE_STATUS'] = -1\n",
    "    df.loc[(df['CASE_STATUS'] == 'REJECTED'), 'CASE_STATUS'] = -1\n",
    "    \n",
    "else:\n",
    "    df = df[~df['case_status'].str.contains(\"Withdrawn\", na=False)]\n",
    "    df.loc[(df['case_status'].str.contains('Certified', na=False)), 'case_status'] = 1\n",
    "    df.loc[(df['case_status'].str.contains('Denied', na=False)), 'case_status'] = -1\n",
    "    \n",
    "    df.loc[(df['wage_offer_unit_of_pay_9089'].str.contains('yr', na=False)), 'wage_offer_unit_of_pay_9089'] = 2\n",
    "    df.loc[(df['wage_offer_unit_of_pay_9089'].str.contains('hr', na=False)), 'wage_offer_unit_of_pay_9089'] = 1\n",
    "    \n",
    "    \n",
    "df = df.apply(lambda x: pandas.to_numeric(x.astype(str).str.replace(',',''), errors='coerce'))\n",
    "\n",
    "# print(df[:10])\n",
    "# indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "# df = df[indices_to_keep]\n",
    "\n",
    "print(df_x)\n",
    "print(df_y)\n",
    "\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 127652\n",
      "['wage_offer_from_9089', 'naics_2007_us_code', 'wage_offer_unit_of_pay_9089', 'country_of_citzenship', 'employer_state', 'employer_state', 'pw_level_9089']\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()  \n",
    "# sample = df.sample(n=10000)\n",
    "\n",
    "# Shuffle\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print('Dataset size: ' + str(df.size))\n",
    "print(df_x)\n",
    "\n",
    "X = df.loc[:, df_x]\n",
    "y = df.loc[:, df_y]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.864924145494\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of feature_names, 7 does not match number of features, 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-a9ca97bae0ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                          \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatuses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                          \u001b[0mfilled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                          special_characters=True)  \n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/tree/export.py\u001b[0m in \u001b[0;36mexport_graphviz\u001b[0;34m(decision_tree, out_file, max_depth, feature_names, class_names, label, filled, leaves_parallel, impurity, node_ids, proportion, rotate, rounded, special_characters, precision)\u001b[0m\n\u001b[1;32m    427\u001b[0m                                  \u001b[0;34m\"does not match number of features, %d\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                                  % (len(feature_names),\n\u001b[0;32m--> 429\u001b[0;31m                                     decision_tree.n_features_))\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;31m# The depth of each node for plotting with 'leaf' option\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of feature_names, 7 does not match number of features, 6"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "statuses=['denied', 'approved']\n",
    "for i in range(1,30 + 1): \n",
    "    clf = tree.DecisionTreeClassifier(max_depth=i, criterion='gini', splitter='best', min_samples_leaf=1)\n",
    "    clf = clf.fit(x_train, y_train)\n",
    "\n",
    "    y_predict = clf.predict(x_test)\n",
    "    print(str(i) + ': ' + str(accuracy_score(y_test, y_predict))) \n",
    "    dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=df_x,  \n",
    "                         class_names=statuses,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "    graph = graphviz.Source(dot_data).view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.864608465955\n",
      "2: 0.858521938565\n",
      "3: 0.85917995957\n",
      "4: 0.85928962673\n",
      "5: 0.858796133531\n",
      "6: 0.865211698487\n",
      "7: 0.85939929389\n",
      "8: 0.85928968086\n",
      "9: 0.864279491539\n",
      "10: 0.864389221851\n",
      "15: 0.865266523046\n",
      "20: 0.861318478215\n",
      "30: 0.862140990938\n",
      "40: 0.86192167466\n",
      "50: 0.862853854544\n",
      "60: 0.862031341821\n",
      "70: 0.861318514301\n",
      "80: 0.861921683682\n",
      "90: 0.861921683682\n",
      "100: 0.861592691223\n",
      "125: 0.860879836639\n",
      "150: 0.861208847141\n",
      "175: 0.861483042106\n",
      "200: 0.861373383968\n",
      "250: 0.861044364444\n",
      "300: 0.860989503799\n",
      "350: 0.861373365925\n",
      "400: 0.860057360003\n",
      "450: 0.860057332938\n",
      "500: 0.86044117702\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "estimators = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30,40, 50, 60, 70, 80, 90, 100, 125, 150, 175, 200, 250, 300, 350, 400, 450, 500]\n",
    "for estimator in estimators:\n",
    "    clf = AdaBoostClassifier(n_estimators=estimator)\n",
    "    scores = cross_val_score(clf, X, y)\n",
    "    print(str(estimator) + ': ' + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='relu', max_iter=1500, alpha=0.001, batch_size=200, hidden_layer_sizes=(150, ), random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train) \n",
    "X_train = scaler.transform(x_train)\n",
    "X_test = scaler.transform(x_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_predict = clf.predict(X_test)\n",
    "print(str(accuracy_score(y_test, y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "# different learning rate schedules and momentum parameters\n",
    "params = [{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0,\n",
    "           'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
    "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
    "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,\n",
    "           'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,\n",
    "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,\n",
    "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'adam', 'learning_rate_init': 0.01}]\n",
    "\n",
    "labels = [\"constant learning-rate\", \"constant with momentum\",\n",
    "          \"constant with Nesterov's momentum\",\n",
    "          \"inv-scaling learning-rate\", \"inv-scaling with momentum\",\n",
    "          \"inv-scaling with Nesterov's momentum\", \"adam\"]\n",
    "\n",
    "plot_args = [{'c': 'red', 'linestyle': '-'},\n",
    "             {'c': 'green', 'linestyle': '-'},\n",
    "             {'c': 'blue', 'linestyle': '-'},\n",
    "             {'c': 'red', 'linestyle': '--'},\n",
    "             {'c': 'green', 'linestyle': '--'},\n",
    "             {'c': 'blue', 'linestyle': '--'},\n",
    "             {'c': 'black', 'linestyle': '-'}]\n",
    "\n",
    "\n",
    "def plot_on_dataset(X, y, ax, name):\n",
    "    # for each dataset, plot learning for each learning strategy\n",
    "    print(\"\\nlearning on dataset %s\" % name)\n",
    "    ax.set_title(name)\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    mlps = []\n",
    "    if name == \"digits\":\n",
    "        # digits is larger but converges fairly quickly\n",
    "        max_iter = 15\n",
    "    else:\n",
    "        max_iter = 400\n",
    "\n",
    "    for label, param in zip(labels, params):\n",
    "        print(\"training: %s\" % label)\n",
    "        mlp = MLPClassifier(verbose=0, random_state=0,\n",
    "                            max_iter=max_iter, **param)\n",
    "        mlp.fit(X, y)\n",
    "        mlps.append(mlp)\n",
    "        print(\"Training set score: %f\" % mlp.score(X, y))\n",
    "        print(\"Training set loss: %f\" % mlp.loss_)\n",
    "    for mlp, label, args in zip(mlps, labels, plot_args):\n",
    "            ax.plot(mlp.loss_curve_, label=label, **args)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "# load / generate some toy datasets\n",
    "\n",
    "plot_on_dataset(X, y, axes.ravel()[0], 'Test')\n",
    "\n",
    "fig.legend(ax.get_lines(), labels, ncol=3, loc=\"upper center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "scores = cross_val_score(clf, X, y)\n",
    "print(str(scores.mean()))\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "scores = cross_val_score(clf, X, y)\n",
    "print(str(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running knn\n",
      "1:uniform: 0.928349479071\n",
      "2:uniform: 0.868579784317\n",
      "3:uniform: 0.895265947724\n",
      "4:uniform: 0.88338512155\n",
      "5:uniform: 0.885212940961\n",
      "10:uniform: 0.87698775361\n",
      "15:uniform: 0.874063242552\n",
      "20:uniform: 0.870224821788\n",
      "25:uniform: 0.871321513434\n",
      "30:uniform: 0.872052641199\n",
      "50:uniform: 0.868762566258\n",
      "1:distance: 0.928349479071\n",
      "2:distance: 0.915554743191\n",
      "3:distance: 0.945348199598\n",
      "4:distance: 0.940778651069\n",
      "5:distance: 0.945530981539\n",
      "10:distance: 0.946993237068\n",
      "15:distance: 0.94735880095\n",
      "20:distance: 0.947176019009\n",
      "25:distance: 0.94735880095\n",
      "30:distance: 0.948638274538\n",
      "50:distance: 0.948638274538\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors\n",
    "\n",
    "print('Running knn')\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "k_vals = [1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 50]\n",
    "\n",
    "for weight in ['uniform', 'distance']:\n",
    "    for k in k_vals:\n",
    "        # we create an instance of Neighbours Classifier and fit the data.\n",
    "        clf = neighbors.KNeighborsClassifier(k, weights=weight)\n",
    "        clf.fit(X, y)\n",
    "        \n",
    "        y_predict = clf.predict(x_test)\n",
    "        print(str(k) + ':' + weight + ': ' + str(accuracy_score(y_test, y_predict))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
