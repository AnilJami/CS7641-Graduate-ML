 \documentclass[h]{article}
\usepackage[margin=0.5in]{geometry}
\usepackage{amsfonts} 
\usepackage{textcomp}
 
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float} 
\usepackage{flafter}
\graphicspath{ {./plots/} }
\usepackage{adjustbox}


\newcommand{\cent}{\textcent \hspace{4pt}}
\title{CS 7641 Machine Learning \\ Assignment 3}
\date{Due Sunday April 1st, 2018 11:59pm}
\author{Philip Bale \\ pbale3}

\begin{document}

\maketitle

\section*{Introduction}  
This assignment explores unsupervised learning and dimensitonality reduction.  
It begins by examining clustering algorithms, specifically k-means and 
expectation maximization.  It then proceeds to cover four dimensionality 
reduction algorithms: principal components analysis, individual components 
analysis, randomized projections, and random forests.  After running these six 
algorithms on the original datasets and observing the results, the results are 
then piped into a neural network learner for further examination.

\subsection*{Datasets chosen}  
The datasets chosen were the same datasets chosen for assignment 1.  The first dataset is the US 
permanent visa dataset.  This dataset is interesting due to its potential to aid in the visa application process 
from a cost and time savings potential. It could also enable confidence in those interested in applying for a 
US permanent visa but doubting their chances of acceptance. 
At the end of the day, the goal is it to try to determine the application result before time, money, 
nd other resources are spent. 
\\  \\
The second dataset is a home sale price prediction dataset taken from an ongoing 
Kaggle competiton.  This dataset is interesting for two primary reasons: real-world applicability and participating in a Kaggle challenge.
 First, modeling home prices is both a difficult and lucrative task. 
 If one can succesfully model home sale prices on large sets of data, he/she can make large amounts of money 
 investing in real estate when he/she detects outliers in listed price vs. what it is expected to sell for. 
 This applies to flipping, investing, and remodeling. 
 Second, the dataset is part of an ongoing Kaggle competition that does not have a winning solution yet.
  By taking part of the competition, the dataset presents the opportunity to work towards a winning solution 
  and advance ones algorithms over time.

\section*{Part 1: Clustering Algorithms}
\subsection*{Introduction}  
Text 

\subsection*{1) k-means clustering}  
\subsubsection*{Overview}
Test
 
% \begin{figure}[H]
%  \minipage{0.49\textwidth}
%      \includegraphics[width=1\textwidth,keepaspectratio]{backprop_nn_1.jpg} 
%      \caption*{Backprop NN Success Rate vs. Iterations} 
%   \endminipage\hfill
%   \minipage{0.49\textwidth}
%      \includegraphics[width=1\textwidth,keepaspectratio]{backprop_nn_time_1.jpg} 
%      \caption*{Backprop NN Training Time} 
%   \endminipage\hfill
%\end{figure}

Text

\subsection*{2) Expectation Maximization}  
\subsubsection*{Overview}
Text

 
\section*{Part 2: Dimensionality Reduction Algorithms}
\subsection*{ Introduction}  
Text

\subsection*{1) Principal Components Analysis (PCA)}  
\subsubsection*{Overview}
Text

\subsubsection*{Analysis}
IText

\subsection*{2) Independent Components Analysis (ICA)}  
\subsubsection*{Overview}
Text

\subsubsection*{Analysis}
IText

\subsection*{3) Randomized Projections}  
\subsubsection*{Overview}
Text

\subsubsection*{Analysis}
IText

\subsection*{4) TODO Choose}  
\subsubsection*{Overview}
Text

\subsubsection*{Analysis}
IText


\section*{Conclusion}  
Todo conclusion

\end{document}